<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mason, Lee, Laa, Cook">

<title>Response to review: Teaching Computers to See Patterns in Scatterplots with Scagnostics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="reply_files/libs/clipboard/clipboard.min.js"></script>
<script src="reply_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="reply_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="reply_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="reply_files/libs/quarto-html/popper.min.js"></script>
<script src="reply_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="reply_files/libs/quarto-html/anchor.min.js"></script>
<link href="reply_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="reply_files/libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="reply_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="reply_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="reply_files/libs/bootstrap/bootstrap-d6a003b94517c951b2d65075d42fb01b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Response to review: Teaching Computers to See Patterns in Scatterplots with Scagnostics</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mason, Lee, Laa, Cook </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p>We thank the editor, associate editors and reviews for their helpful comments to improve our paper. Here we provide a point-by-point explanation of how we have responded to each suggestion. Reviewer comments are in <em>italics</em> and our response is in <strong>bold</strong>.</p>
<section id="review-1" class="level1">
<h1><em>Review 1</em></h1>
<section id="general" class="level2">
<h2 class="anchored" data-anchor-id="general"><em>General</em></h2>
<p><em>This paper concerns the CRAN package cassowaryr for calculating scagnostics. This package is an alternative to the existing scagnostics package, with the code now written in R, and some improvements made to the definition of the scagnostics themselves. Scagnostics are a very useful tool for screening scatterplots, and their existing implementation in the scagnostics package are known to have some flaws. This new package, particularly as it is tidyverse friendly, could help promote scagnostics as a tool to the R community.</em></p>
<p><em>Very little of the paper is given over to the package, its design and implementation. No comparison is made to the original scagnostics package.Though binning is mentioned in the paper, it is not available in the package. Presumably if it did include binning, then the original scagnostic definitions should be used. Presumably also, the new implementation of scagnostics is a lot slower than the original due to the R vs C++ implementation and the lack of binning, though this is not discussed.</em></p>
<p><strong>Want comparison to scagnostics package. Package had issues with it. Hadley has similar code in the binostics package that has an underlying one. Could try doing that. Not sure how time comparison will work (since not in R) also no binning so hard to compare.</strong></p>
<p><em>Much of the paper is given over to four examples, though code is not given. While the examples are very interesting, four examples is excessive. Particularly the third example is a bit “meta” (scagnostics of scagnostics…) and is rather complex for the current paper.</em></p>
<p><strong>See detailed comments</strong></p>
<p><em>RJ papers concerning a package should include more than just a little code demonstrating the package.</em></p>
<p><strong>TO DO</strong></p>
</section>
<section id="detailed-comments" class="level2">
<h2 class="anchored" data-anchor-id="detailed-comments"><em>Detailed comments</em></h2>
<p><em>Page 1: There is extended discussion of dimension reduction in the first paragraph. This is not directly relevant to the topic at hand, and should be reduced to a sentence or two.</em> <strong>We agree and have reduced that section to a few sentences.</strong></p>
<p><em>The word “biplots” is used a few tines as a replacement for “scatterplot”. This usage is incorrect: a biplot is where you show variables and cases on the same plot.</em> <strong>We agree and have made the changes.</strong></p>
<p><em>Page 1: Par 2 scagnostics produce a hierarchy of variable pairs, not variables. Tukey reference in this paragraph needs fixing.</em> <strong>We agree and have made the changes.</strong></p>
<p><em>Page 2: “Before any of the scagnostics are calculated, outlying points are removed.” Surely not true for outlying, monotonic and other association-based measures?</em> <strong>Outlying uses both the raw MST and the MST with the outliers removed. Monotonic and other association-based measures use the MST with the outlying points removed. We remove outliers because we want the scagnostics (including association-based measures) to be robust. We also want to maintain consistency and make sure all the scagnostics are computed on the same underlying plot, with outlying being the only exception. While we remove the outliers for all measures (excluding outlying), this is only by default. If users want to compute the scagnostics on the raw data (i.e.&nbsp;no outlier removal) they can set out.rm=FALSE in the calc_scags function. We edited this section of the paper to make the details of this pre-processing step clearer.</strong></p>
<p><em>Page 2 “to range” -&gt; “to range in”</em> <strong>We agree and have made the changes.</strong></p>
<p><em>Page 4: description of sparse: “Identifies if the data is sporadically located on the plane.” This does not make sense to me.</em> <strong>Good point. We changed the wording so it is more in line with the original explanation from the Scacnostic Distributions paper. It now reads “Identifies if the data is confined to a small number of locations on the plane”.</strong></p>
<p><em>Page 5: For dcor include a reference to distance corrrelation.</em> <strong>Good catch. We have added the reference to distance correlation.</strong></p>
<p><em>Page 5: In the discussion of Figure 4, you point out problems with the original scagnostic measures. When comparing this to Figures 4 and 6 of the Wilkinson and Wills (2008) paper, it would seem more than a suspicion that the defects evident in Figure 4 are due to binning. Why not verify this using the scagnostics package?</em> <strong>If easy we can check this with the binostics package.</strong></p>
<p><em>Page 6: Table 1: As regards stringy, it says “This measure rarely drops below 0.5”, but this does not agree with the Wilkinson and Wills figures 5 and 6.</em> <strong>We agree that this does not agree with the Wilksinson and Wills figures 5 and 6. This is why it was listed as an “issue” with our calculation of the measure and we explcitly mention this issue did not exist in the binned version of the statistics. The issues table outlines problems with the cassowaryr computations of the measures. Some of them overlap with issues that might arise in the Wilkinson (2008) calculations, but some of them are unique to the cassowaryr computation.</strong></p>
<p><em>Page 7: “in contention with” -&gt; “align with” ?</em> <strong>We are saying that the measures do not align with human intuition, but agree that phrase is needlessly complicated. We changed it to “conflicts with”</strong></p>
<p><em>Page 7: The discussion of Figure 5 is confusing. The “line” plot does not score 1 on either striated measure. The caption says “line and vlines have the same result” but this is not evident in the display.</em> <strong>This is actually an issue caused by interp package that I missed (and also why I am eternally annoyed that Albrech would not use the tRiad package) - I suspect the noise added in the line plot causes the MST edges to go beyond the error tollerance for the striated measure. - I might be able to fix this by tinkering with the error tollerance in the striated measure - Good catch by the reviewer tbh</strong></p>
<p><em>Page 13: Figure 7 In the caption it refers to the black edge, do you mean green?</em> <strong>We agree and have made the changes.</strong></p>
<p><em>Page 13: In the discussion associates with Figure 8, you mention picking scatterplots with a low scagnostic value. Why would you do this? Up to now, it seems you were looking for high scagnostic scores.</em></p>
<p><strong>Up to this point in the paper, we were assessing the scagnostics with the features data, so we were hoping the scatter plots that typified each feature would have a high value on its respective scagnostic (e.g.&nbsp;the clusters plot would have the highest clumpy measure) for each index (i.e.&nbsp;trying to design ).</strong></p>
<p><strong>In the AFLW example, we are using the scagnostics to explore the data and identify interesting structures. For this reason, we are no longer looking for high or low values per se, but rather unusual scagnostic values. This is also done in the Scagnostics Distributions paper (Figure 2). The idea is that these outliers should identify plots that are visibly dissimilar from other scatter plots in the data set and might have an unexpected structure.</strong></p>
<p><strong>We agree that this approach should have been mentioned earlier in the paper. We have now included a sentence in the third paragraph of the introduction that mentions using a SPLOM of scagnostic values as an EDA technique. We also included a more detailling introduction to the approach in the first paragraph of the AFLW example.</strong></p>
<ul>
<li><em>“should be in package documentation”</em> <strong>To DO note from meeting with Ursula, not sure what this means</strong></li>
</ul>
<p><em>In Figure 9 top row the coloured dots are hard to pick out. I suggest you make them bigger.</em> <strong>We agree. We made them bigger and added a black outline.</strong></p>
<p><em>In Figure 10. explain that the blue/red dots represent the same plots in the three scagnostic plots. Again coloured dots are small, suggest making them bigger.</em> <strong>We agree. We made them bigger and added a black outline, and added a sentance in Figure 10 and Figure 11 connecting the plots.</strong></p>
<p><em>Figure 11: I would not describe the green plot (alpha vs time) as exhibiting clumpiness.</em> <strong>While they are not elliptical, the plot does show two disconnected regions of higher density, and we would expect the clumpy index to be sensitive to this gap pattern, based on the definition in terms of edge lengths in the MST.</strong></p>
<p><em>Page 13: “The best way to identify interesting scatter plots is to construct a large interactive SPLOM” This statement needs more justification. Why a SPLOM as opossed to another display of multivariate data? I would like to see this interactive SPLOM included (and suggest dropping the final two examples.) It would be nice if your package included some functionality to identify interesting scatterplots, beyond top_pairs, which is not used in any of the examples.</em> <strong>We agree that the use of the word “best” was poor phrasing on our part. We realised the information in this paragraph is now included in the introduction to the AFLW example, so we just removed it. Even though the interactive SPLOM is more manageable than the original data set, the entire interactive SPLOM would take up a lot of space and be difficult to read if fit within the bounds of the paper. The purpose of cassowaryr is largely to calculate scagnostics and it is designed to fit within an ecosystem that is better equipt to do the functionality. Since the data is returned in a tidy data format, it can be fed into outlier detection or visualisation tools directly by the user. For example, the calc_scags_wide function gives the scagnostics back as a wide data frame, which can be passed into ggpairs if the user wants to make a SPLOM. Any functionality that does exist in the package was largely for our own benefit to avoid copy and pasting code. If you have any suggestions for specific functionality additions, we would be happy to look into implementing them.</strong></p>
<p><em>Page 14: “The splines vs dcor plot tells us that there is a strong linear relationship”. But both of these are measures of non-linear association.</em> <strong>We agree, and rephrased it to functional relationship.</strong></p>
<p><em>Page 14: the scagnostics for the physics data are computed on a sample of 200 out of ~10K observations. Binning would be very useful in this case. Presumably outliers could easily be “lost” in such a small sample.</em> <strong>We are looking for the main patterns and not for outliers, but we agree that if binning is available then it could be the full data. We have adjusted the phrasing to clarify this.</strong></p>
<p><em>Page 16: The statement about LDA assumptions is incorrect. In any case, the LDA/QDA discussion is not directly relevant.</em> <strong>The statement in the paper is correct. It was used to make an analogy, but we have taken this out because it clearly did not help the reviewer.</strong></p>
<p><em>Page 17: PCA regression description is not accurate, as the dimension reduction steps and regression are performed independently. Suggest removing the discussion about pca, it is not really relevant and is confusing.</em> <strong>Again, not incorrect, but we have removed reference to PCA from the paper.</strong></p>
<p><em>Page 17: “Macroeconomic series tend to have moderate curvature and varied trend, while microeconomic series tend to have strong trends and varied curvature.” How is this statement supported by figure 13, where trends look about the same between micro and micro series? Then the Figure caption says “The way that trend strength is calculated, on closer inspection, could lead to describing jagginess.” which seems strange and could do with more explanation.</em> <strong>We have removed this example. It is complicated and not necessary to illustrate the package.</strong></p>
<p><em>This third example is scagnostics calculated on features of time series rather than time series themselves. This is an interesting idea, but needs to be motivated. Also the code provided for this example is not self-contained, as the code for calculation of the feasts indices is not provided.</em> <strong>See note above. Exmaple is removed</strong></p>
<p><em>Page 19: In the WBI example (Fig 14) the top clumpy2 value is (close to) 1, yet the scatterplot is not what I’d describe as clumpy. Is this a problem with the index?</em> <strong>Yes, what we imagine to be structure based on the index name does not always match what the structure looks like to our eyes.</strong></p>
<p><em>Page 19: “This tells us that in the WBI data, the relationships between variables is dominated by outliers (as noted by the high values on the outlying scagnostic, and to some extent also skewed and stringy), and no relationship (given bu the high values on convex).” There is something wrong with this sentence.</em> <strong>(Di problem)</strong></p>
<p><em>Some references in the paper use the author’s first initial/name.</em> <strong>We agree and have made the changes.</strong></p>
<p><em>Chapter 10 of PhD thesis of Adam Rahman is relevant: https://core.ac.uk/download/pdf/158325743.pdf</em> <strong>Ursula looked, I can also check. Changed some definitions and stuff. Might want to reference these types of adjustment exist and have been done before</strong></p>
</section>
<section id="code" class="level2">
<h2 class="anchored" data-anchor-id="code">Code</h2>
<p><em>For the features data, would be useful if these patterns could be generated with different amounts of noise.</em> <strong>If functional dependence it is easy, but for the other ones it is not clear how you would add noise. Can simulate first then add noise on top if desperate.</strong></p>
<p><em>Data with NA generates an error.</em> <strong>Should check this, shouldnt happen, will fix if it does.</strong></p>
<p><em>The function calc_scags has an argument euclid which is not explained.</em> <strong>So true. Either explain or remove I guess.</strong></p>
<p><em>calc_scags has no option to bin the data.</em> <strong>You can’t. Should make it clearer in the paper there is no binning.</strong></p>
<p><em>The package has no vignettes, beyond the README</em> <strong>There is a vignette <a href="https://numbats.github.io/cassowaryr/articles/cassowaryr.html">here</a>. Do they want it longer?</strong></p>
</section>
</section>
<section id="review-2" class="level1">
<h1><em>Review 2</em></h1>
<section id="general-1" class="level2">
<h2 class="anchored" data-anchor-id="general-1"><em>General</em></h2>
<p><em>The article outlines the most common scagnostic measures and introduces improvements to fix undesirable behaviour in the patterns they highlight and make them more intuitive. After discussing the implementation in <code>cassowaryr</code>, several examples and tests demonstrate the use of scagnostics and <code>cassowaryr</code> in practice.</em></p>
<p><em>This paper is a welcome addition to the literature on scagnostics. By nature, these are applied measures, but some previous works do not present them in an applied way. Here, an appropriate amount of theory is presented with an outline of a software package and many worked examples to emphasize the package’s intended use. The equal emphasis on all three aspects of scagnostics is a perfect accompaniment to a package intended for widespread use and a nice way to introduce the topic.</em></p>
<p><em>There are several major areas which could be improved, however. First, major edits of the writing are required. Simple typos are present in most sections and many of the sentences have awkward wording, detracting from the tutorial the paper intends to deliver. The detailed comments below identify many of these but there are likely others which have been missed.</em></p>
<p><em>A second major issue is the introduction of the scagnostics. While the choice to make it brief and minimally theoretical is appropriate, for several of the scagnostics this unfortunately makes the formulas presented confusing or even impossible to understand. In particular, dcor is presented without the necessary context to understand the notation used in its definition. A better balance must be struck between context and brevity when defining the measures if formulas are to be provided in their definitions. Alternatively, more detailed descriptions of these scagnostics could be combined with references to the appropriate papers giving their formulas.</em></p>
<p><strong>Come back to this after detailed comments</strong></p>
<p><em>Finally, the title seems problematic. It is an awkward way to express the content of the article and is likely to misleading many readers. Given the pace and high publicity of research in computational statistics around ‘teaching’ computers to do tasks such as object recognition, speech, and translation, it seems unwise to use the term for an article which presents methods designed for human interpretation. Emphasizing the computer and claiming to ‘teach’ it to ‘see’ with scagnostics is exaggerated and gives the wrong impression of the article’s content.</em> <strong>I agree, although it was less of an issue when we originally wrote the paper (four years ago). The title has been changed.</strong></p>
</section>
<section id="detailed-comments-1" class="level2">
<h2 class="anchored" data-anchor-id="detailed-comments-1"><em>Detailed comments</em></h2>
<p><em>Consider reordering Table 1 and Figure 3 for clarity, skipping one to read the other breaks the flow of the article.</em> <strong>We feel that swapping the table and the figures would only end up shifting this jumping back and forth issue from Table 1 to Figures 3 and 4. As there is a discussion of the figures in the paragraph, we feel that this version of the paper would be less cohesive.</strong></p>
<ul>
<li><p><em>Clean up references and make them consistent (some use first names, others use abbreviations, still others a mix of both: this makes for jarring and confusing reading)</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>last paragraph of introduction, line 5, “scagnostics” should be “scagnostics’”</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>last sentence of first paragraph of introduction: wording feels clunky, maybe change ‘is’ to ‘are’ and drop the ‘even’</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>outlying description on page 4: ‘dataset’ is missing an article (‘the dataset’, perhaps)</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>stringy description on page 4: would benefit from explicitly stating the vertices are from the MST, all other descriptions are explicit about using the MST, A, or C</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>introduction of DCor on page 5: must define and describe what a_{kl} and b_{kl} are, otherwise the equation should not be provided and left in references</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>page 5, first sentence of Checking Scagnostics Calculations: missing apostrophe on “package’s”</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>skewed description in table 1 seems to be missing ‘s’ on the verbs, “reguarding” is spelled incorrectly rather than “regarding”</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>page 7: how strict is the restriction around 180 &amp; 90 degrees?</em> <strong>We agree. To make it clear we have changed the sentence to: ____</strong></p></li>
<li><p><em>page 7: last paragraph is highly confusing… what is the ‘lines’ scatterplot referenced and how does it differ from ‘vlines’? The later comments make sense but the whole discussion is made confusing by errors like this</em> <strong>Issue caused by the error restriction around 180 degrees</strong></p></li>
<li><p><em>page 7, last paragraph: ‘axis’ should be ‘axes’, ‘observation’ should be plural, ‘version’ should be plural</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>page 9, clumpy adjusted point 3: reword to be ‘vertices with adjacent angles forming’</em> <strong>I think I specifically tried to use the terminology from the original scagnostics paper to keep it consistent. Check that. But also, if the reviewer was confused, we should probably reword this so it is clearer.</strong></p></li>
<li><p><em>table 3 on page 11 has some typos: ‘stirated’, ‘scagnositc’, etc.</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>page 11, calculate functions: ‘…each function is a scagnostics’ should not be pluralized</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>page 12, making summaries: ‘summarise’ should be ‘summaries’</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>page 13, end of page: spaces missing in ‘…SPLOM of the scagnostic values,each…” and ’Plot2’</em> <strong>We agree and have made the changes.</strong></p></li>
<li><p><em>page 14, non-linear shapes section: first sentence has somewhat awkward wording, the use of ‘that’s’ is incorrect</em> <strong>We agree and have rephrased the sentence.</strong></p></li>
<li><p><em>page 15: ‘except’ should be ‘expect’</em> <strong>We fixed the typo.</strong></p></li>
<li><p><em>page 16: ‘…from each groups’ should be “…from each group’s”</em> <strong>Di section</strong></p></li>
<li><p><em>page 17: ‘…utilising the scagnostics’ should be “…utilising the scagnostics’”</em> <strong>Di section</strong></p></li>
<li><p><em>page 18, figure 13 caption: by ‘jagginess’, is ‘jaggedness’ meant?</em> <strong>Di section</strong></p></li>
<li><p><em>page 19, figure 14: the description of the figure states that only a single pair ranked highest on striated2, but two points can be observed in the plot, what is the cause of this discrepancy? The caption also seems to state something different</em> <strong>Di section</strong></p></li>
<li><p><em>page 19: ‘…between variables is…’ should be ‘…between variables are…’</em> <strong>Di section</strong></p></li>
<li><p><em>page 19: ‘bu’ should be ‘by’</em> <strong>Di section</strong></p></li>
<li><p><em>page 19: missing apostrophe on ‘packages’ (should be ‘package’s’)</em> <strong>We agree and have made the changes.</strong></p></li>
</ul>
</section>
<section id="code-1" class="level2">
<h2 class="anchored" data-anchor-id="code-1">Code</h2>
<p><em>Usability:</em></p>
<p><em>The documentation is thin, but this is appropriate with the vignette provided and the simplicity of the intended workflow. This package is highly user-friendly and accessible.</em> <strong>Thanks</strong></p>
<p><em>Code:</em></p>
<p><em>While the code is generally robust to different patterns in the data, there seem to be a few edge cases where its behaviour is undesirable and it returns strange errors. Using it on the <code>anscombe_tidy</code> data,</em></p>
<pre><code>for (ii in 1:4)
   test[[ii]] &lt;- calc_scags(anscombe_tidy$x[anscombe_tidy$set == ii],
                            anscombe_tidy$y[anscombe_tidy$set == ii])</code></pre>
<p><em>for example, gives the error: “In alphahull::areaahull(y) : Problem in area computation (Returns NA)” for the last two patterns. The table returned either contains only NAs or has no rows. It seems the removal of the outlying points in these two patterns leads to an alpha hull around a perfectly straight line and this causes the code to fail. In addition to this failure, the error provided is somewhat uninformative to the user, and could only be deciphered here because of the simplicity of the data.</em> <strong>Should check this, shouldnt happen, will fix if it does.</strong></p>
<p><em>The functions are clearly named, and the code is well-organized and understandable.</em></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>