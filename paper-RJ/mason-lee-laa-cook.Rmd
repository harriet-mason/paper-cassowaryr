---
title: "Teaching Computers to See Patterns in Scatterplots with Scagnostics"
abstract: >
  An abstract of less than 150 words.
draft: true
author:  
  # see ?rjournal_article for more information
  - name: Harriet Mason
    affiliation: Monash University
    address:
    - Department of Econometrics and Business Statistics
    - Melbourne, Australia
    url: https://www.britannica.com/animal/quokka
    orcid: 0000-1721-1511-1101
    email:  hmas0003@student.monash.edu
  - name: Stuart Lee
    url: https://stuartlee.org
    email: stuart.andrew.lee@gmail.com
    orcid: 0000-0003-1179-8436
    affiliation: Genentech
  - name: Ursula Laa
    affiliation: University of Natural Resources and Life Sciences
    address:
    - Institute of Statistics
    - Vienna, Austria
    url: https://uschilaa.github.io
    orcid: 0000-0002-0249-6439
    email:  ursula.laa@boku.ac.at
  - name: Dianne Cook
    affiliation: Monash University
    address:
    - Department of Econometrics and Business Statistics
    - Melbourne, Australia
    url: https://dicook.org
    orcid: 000-0002-3813-7155
    email:  dicook@monash.edu
type: package
creative_commons: CC BY
output: 
  rjtools::rjournal_web_article
bibliography: mason-lee-laa-cook.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, 
  warning = FALSE, 
  message = FALSE)
```

```{r load-libraries}
library(cassowaryr)
library(GGally)
library(plotly)
library(tidyverse)
library(plotly)
library(patchwork)
library(knitr)
library(ggthemes)
library(ggimage) #for the visual table
```

# Introduction

Visualising high dimensional data is often difficult and requires a trade-off between the usefulness of the plots and maintaining the structures of the original data. Scagnostics (scatterplot diagnostics) are a set of visual features that can be used to identify interesting and abnormal scatterplots, and thus give a sense of priority to the variables we choose to visualise. This proposal will discuss the creation of an R package that will provide a user-friendly method to calculate these scagnostics. The package will be tested on datasets with known interesting visual features to ensure the scagnostics are working as expected, before finally being used to explore and describe a time series dataset.

As the number of dimensions in a dataset increases, the process of visualising its structure and variable dependencies becomes more tedious. This is because the number of possible pairwise plots rises exponentially with the number of dimensions. Datasets like Anscombe's quartet [@anscombe] or the datasaurus dozen [@datasaurpkg] have been constructed such that each pairwise plot has the same summary statistics but strikingly different visual features. This design is to illustrate the pitfalls of numerical summaries and the importance of visualisation. This means that despite the issues that come with increasing dimensionality, visualisation of the data cannot be ignored. Scagnostics offer one possible solution to this issue.

The term scagnostics was introduced by John Tukey in 1982 [@tukey]. Tukey discusses the value of a cognostic (a diagnostic that should be interpreted by a computer rather than a human) to filter out uninteresting visualisations. He denotes a cognostic that is specific to scatter plots a scagnostic. Up to a moderate number of variables, a scatter plot matrix (SPLOM) can be used to create pairwise visualisations, however, this solution quickly becomes infeasible. Thus, instead of trying to view every possible variable combination, the workload is reduced by calculating a series of visual features, and only presenting the outlier scatter plots on these feature combinations. 

There is a large amount of research into visualising high dimensional data, most of which focuses on some form of dimension reduction. This can be done by creating a hierarchy of potential variables, performing a transformation of the variables, or some combination of the two. Unfortunately none of these methods are without pitfalls. Linear transformations are subject to crowding, where low level projections concentrate data in the centre of the distribution, making it difficult to differentiate data points [@crowding]. Non-linear transformations often have complex parameterisations, and can break the underlying global structure of the data, creating misleading visualisations. While there are solutions within these methods to fix these issues such as a burning sage tour for crowding [@burningsage] or liminal package for maintaining global structure [@liminal] all these methods still involve some transformation of the data. Scagnostics gives the benefit of allowing the user to view relationships between the variables in their raw form. This means they are not subject to the linear transformation issue of crowding, or the non-linear transformation issue of misleading global structures. That being said, only viewing pairwise plots can leave our variable interpretations without context.  Methods such as those shown in *ScagExplorer* [@scagexplorer] try to address this by visualising the pairwise plots in relation to the scagnostic measures distribution, but ultimately the lack of context remains one of the limitations of using scagnostics alone as a dimension reduction technique.

Scagnostics are not only useful in isolation, they can be applied in conjunction with other techniques to find interesting feature combinations of the transformed variables. The tourr projection pursuit currently uses a selection of scagnostics to identify interesting low level projections and move the visualisation towards them [@tourrpp]. Since scagnostics are not dependent on the type of data, they can also be used to compare and contrast scatter plots regardless of the discipline. In this way, they are a useful metric for something like the comparisons described in *A self-organizing, living library of time-series data*, which tries to organise time series by their features instead of on their metadata [@sots]. 

Several scagnostics have been previously defined in *Graph-Theoretic Scagnostics* [@scag], which are typically considered the basis of the visual features. They were all constructed to range [0,1], and later scagnostics have maintained this scale. The formula for these measures were revised in *Scagnostic Distributions* and are still calculated according to this paper [@scagdist]. In addition to the main nine, the benefit of using two additional association scagnostics were discussed in Katrin Grimm's PhD thesis [@Grimm]. These two association measures are also used in the tourr projection pursuit [@tourrpp]. 

There are two existing scagnostics packages, *scagnostics* [@scagdist] and the archived package *binostics* [@binostics]. Both are based on the original C++ code from *Scagnostic Distributions* [@something], which is difficult to read and difficult to debug. Thus there is a need for a new implementation that enables better diagnosis of the scagnostics, and better graphical tools for examining the results. 

This paper describes the R package, `cassowaryr` that computes the currently existing scagnostics, and adds several new measures.  The paper is organised as follows. The next section explains the scagnostics. This is followed by a description of the implementation. Several examples using collections of time series and XXX illustrate the usage. 

# Scagnostics

## Building blocks for the graph-based metrics

In order to capture the visual structure of the data, graph theory is used to calculate most of the scagnostics. The pairwise scatter plot is re-constructed as a graph with the data points as vertices and the edges are calculated using Delaunay triangulation. In the package this calculation is done using the alphahull package [@alphahull] to construct an object called a scree. This is the basis for all the other objects that are used to calculate the scagnostics (except for monotonic, dcor and splines which use the raw data). The graph (screen object) is then used to construct the three key structures on which the scagnostics are based; the convex hull, alpha hull and minimum-spanning tree (MST).

- **Convex hull:** The outside vertices of the graph, connected to make a convex polygon that contains all points. It is constructed usnig the tripack package.

- **Alpha hull:** A collection of boundaries that contain all the points in the graph. Unlike the convex hull, it does not need to be convex. It is calculated using the alphahull package [@alphahull].

- **MST:** the minimum spanning tree, i.e the smallest distance of branches that can be used to connect all the points. In the package it is calculated from the graph using the igraph package [@igraph].  

```{r building-blocks, out.height = "30%", out.width = "100%", fig.cap = "The building blocks for graph-based scagnostics", eval=FALSE}
knitr::include_graphics("figures/draw1.png")
```

```{r building-blocks2, width = 150, height = 50, out.width = "100%", fig.cap = "The building blocks for graph-based scagnostics"}
library(alphahull)
data("features")
nl <- features %>% filter(feature == "nonlinear2")
d1 <- draw_convexhull(nl$x, nl$y) +
  ggtitle("a. Convex hull") +
  xlab("") + ylab("") +
  theme_void() +
  theme(aspect.ratio=1, axis.text = element_blank())
d2 <- draw_alphahull(nl$x, nl$y) +
  ggtitle("b. Alpha hull") +
  xlab("") + ylab("") +
  theme_void() +
  theme(aspect.ratio=1, axis.text = element_blank())
d3 <- draw_mst(nl$x, nl$y) +
  ggtitle("c. Min. span. tree") +
  xlab("") + ylab("") +
  theme_void() +
  theme(aspect.ratio=1, axis.text = element_blank())
d1 + d2 + d3
```

## Graph-based scagnostics

The nine scagnostics defined in *Scagnostic Distributions* are detailed below with an explanation, formula, and visualisation.  We will let *A*= alpha Hull *C*= convex hull, *M* = minimum spanning tree, and *s*= the scagnostic measure. Since some of the measures have some sample size dependence, we will let *w* be a constant that adjusts for that.  

- **Convex:** Measure of how convex the shape of the data is. Computed as the ratio between the area of the alpha hull (A) and convex hull (C).  
  
$$s_{convex}=w\frac{area(A)}{area(C)}$$  

![](figures/drawconvex.png)  



- **Skinny:** A measure of how "thin" the shape of the data is. It is calculated as the ratio between the area and perimeter of the alpha hull (A) with some normalisation such that 0 correspond to a perfect circle and values close to 1 indicate a skinny polygon.  

$$s_{skinny}= 1-\frac{\sqrt{4\pi area(A)}}{perimeter(A)}$$  

![](figures/drawskinny.png)

- **Outlying:** A measure of proportion and severity of outliers in dataset. Calculated by comparing the edge lengths of the outlying points in the MST with the length of the entire MST.  

$$s_{outlying}=\frac{length(M_{outliers})}{length(M)}$$  

![](figures/drawoutlying.png)  

- **Stringy:** This measure identifies a "stringy" shape with no branches, such as a thin line of data. It is calculated by comparing the number of vertices of degree two ($V^{(2)}$) with the total number of vertices ($V$), dropping those of degree one ($V^{(1)}$).  

$$s_{stringy} = \frac{|V^{(2)}|}{|V|-|V^{(1)}|}$$  

![](figures/drawstringy.png) 

- **Skewed:** A measure of skewness in the edge lengths of the MST (not in the distribution of the data). It is calculated as the ratio between the 40% IQR and the 80% IQR, adjusted for sample size dependence.  

$$s_{skewed} = 1-w(1-\frac{q_{90}-{q_{50}}}{q_{90}-q_{10}})$$  

![](figures/drawskewed.png)  

- **Sparse:**  Identifies if the data is sporadically located on the plane. Calculated as the 90th percentile of MST edge lengths.

$$s_{sparse}= wq_{90}$$

![](figures/drawsparse.png)


- **Clumpy:** This measure is used to detect clustering and is calculated through an iterative process. First an edge J is selected and removed from the MST. From the two spanning trees that are created by this break, we select the largest edge from the smaller tree (K). The length of this edge (K) is compared to the removed edge (J) giving a clumpy measure for this edge. This process is repeated for every edge in the MST and the final clumpy measure is the maximum of this value over all edges.  

$$\max_{j}[1-\frac{\max_{k}[length(e_k)]}{length(e_j)}]$$  
![](figures/drawclumpy.png) 


- **Striated:** This measure identifies features such as discreteness by finding parallel lines, or smooth algebraic functions. Calculated by counting the proportion of acute (0 to 40 degree) angles between the adjacent edges of vertices with only two edges.  

$$\frac1{|V|}\sum_{v \in V^{2}}I(cos\theta_{e(v,a)e(v,b)}<-0.75)$$  
![](figures/drawstriated.png) 

  
- **Monotonic:** Checks if the data has an increasing or decreasing trend. Calculated as the Spearman correlation coefficient, i.e. the Pearson correlation between the ranks of x and y.  

$$s_{monotonic} = r^2_{spearman}$$
![](figures/drawmonotonic.png)  

## Association-based scagnostics 

The two additional scagnostics discussed by Katrin Grimm are described below.


- **Splines:** Measures the functional non-linear dependence by fitting a penalised splines model on X using Y, and on Y using X. The variance of the residuals are scaled down by the axis so they are comparable, and finally the maximum is taken. Therefore the value will be closer to 1 if either relationship can be decently explained by a splines model.

$$s_{splines}=\max_{i\in x,y}[1-\frac{Var(Residuals_{model~i=.})}{Var(i)}]$$

![](figures/drawsplines.png) 


- **Dcor:** A measure of non-linear dependence which is 0 if and only if the two variables are independent. Computed using an ANOVA like calculation on the pairwise distances between observations. 

$$s_{dcor}= \sqrt{\frac{\mathcal{V}(X,Y)}{\mathcal{V}(X,X)\mathcal{V}(Y,Y)}}$$  
where
$$\mathcal{V}
(X,Y)=\frac{1}{n^2}\sum_{k=1}^n\sum_{l=1}^nA_{kl}B_{kl}$$  
where
$$A_{kl}=a_{kl}-\bar{a}_{k.}-\bar{a}_{.j}-\bar{a}_{..}$$
$$B_{kl}=b_{kl}-\bar{b}_{k.}-\bar{b}_{.j}-\bar{b}_{..}$$

![](figures/drawdcor.png)

## Checking the scagnostics calculations 

Once we have written the scagnsotics by their definition and working functions that correctly calculate them, we can assess how well they identify the visual features of scatter plots. To assess how well the scagnostics differentiate plots, we have creates a dataset called "features" (that is also in the Cassowaryr package) that contains a series of interesting and unique scatter plots which we can run our scagnsotics on.

```{r, Data Cleaning and Scag Calculation, include=FALSE}
# edit data
set.seed(29171741)
# variables to use in making the plots

# generate circle data
theta <- runif(150, 0, 2*pi)
r1 <- rbeta(150, 3, 2)
r2 <- rbeta(150, 10, 1)

# generate striated x
vlx <- sample(c(1,2,3), 150, replace=TRUE)

# generate outlying data
outx <- c(rnorm(147, 0,1), 0, 10, 10)
outy <- c(rnorm(147, 0,1), 10, 10, 0)

# formula data
liney <- 2*theta + 2
nonline <- 2*theta^3 - 10*theta^2 - 5*theta +  8


extrafeatures <- tibble(feature = c(rep("disk", 150), rep("ring", 150), rep("vlines", 150), 
                                    rep("outliers2", 150), rep("line", 150), rep("nonlinear1", 150)),
                        x = c(r1*cos(theta), r2*cos(theta), vlx, outx, theta, theta),
                        y = c(r1*sin(theta), r2*sin(theta), theta, outy, liney, nonline)
                        )

#combine with current features
bigfeatures <- bind_rows(features, extrafeatures)

# run scagnostics
features_scagnostics_wide <- bigfeatures %>%
  group_by(feature) %>%
  summarise(calc_scags(x,y)) %>%
  select(-clumpy_adjusted)

#long version of
features_scagnostics_long <- features_scagnostics_wide %>%
  pivot_longer(cols=outlying:dcor, names_to = "scagnostic")

#transpose of wide feature scagnostics table
t_features_scagnostics_wide <- features_scagnostics_long %>%
  pivot_wider(names_from = "feature")


```

```{r, Features plot}
#plot them
featplot <- ggplot(bigfeatures, aes(x,y,colour=feature))+
  geom_point()+
  theme_minimal() + 
  facet_wrap(~feature, scales="free")
featplot
```
If we run these


```{r, Scatter Plots as images}

#set theme so all scatter plots in table match
plot_theme <-  theme_classic() + #theme_minimal() + 
  theme(aspect.ratio=1, axis.title=element_blank(), axis.text = element_blank(), 
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA, size=4),
        legend.position = "none"
        )

#save scatter plots as images
plots <- unique(bigfeatures$feature)

for (i in seq(length(plots))){
  holdplot <- bigfeatures %>% 
    filter(feature==plots[i]) %>% 
    ggplot(aes(x,y, size=2))+ geom_point() + plot_theme
  ggsave(paste0("figures/", plots[i], ".png"),holdplot)
}

```


```{r, Table of Plots}

# edit data frame
plot_data <- features_scagnostics_long %>%
  mutate(plotad = paste0("figures/", feature, ".png"))

# which plots to include in visual table
whichplots <- function(scag, feature){
  pad = FALSE
  if(all(scag=="convex", feature %in% c("discrete", "outliers", "line"))){
    pad = TRUE
  }
  if(all(scag=="skinny", feature %in% c("line", "l-shape", "disk"))){
    pad = TRUE
  }
  if(all(scag=="outlying", feature %in% c("outlying2", "outliers"))){
    pad = TRUE
  }
  if(all(scag=="stringy", feature %in% c("nonlinear_line", "outliers"))){
    pad = TRUE
  }
  if(all(scag=="striated", feature %in% c("vlines", "weak"))){
    pad = TRUE
  }
  pad
}

# Make Visual Table
# Data
plot_data <- plot_data %>%
  group_by(scagnostic, feature) %>%
  mutate(doplot = whichplots(scagnostic, feature)) %>%
  ungroup() %>%
  filter(doplot==TRUE)

# so i dont have to keep adjusting the image size
s <- length(unique(plot_data$feature))

# plot
visual_table <- ggplot(plot_data, aes(x=value , y=scagnostic))+
  geom_image(aes(image = plotad), size=1/s, by="width") +
  theme_classic() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        legend.position="none") +
  xlim(-0.1,1.1) +
  scale_size_identity()+
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
)
ggsave("figures/visual_table.png", visual_table, width=10, height=10)

```

While the scagnsotics can be shown to sort reasonably well within themselves, is is entirely different to correctly sort the scatter plots by what you would visually conclude.

# Software implementation

## Installation

## Data sets

## Functions

### Scagnostics functions

### Drawing functions

### Summary functions


## Tests

# Examples

## Collections of time series

GOAL: Use scagnostics to find difference in shapes between groups. Here we want to first use features to describe a time series, and then secondly choosing pairs of features where there is the biggest difference between groups according to a scagnostic.

A paragraph describing the compenginets data

Analysis notes:

- A big collection of time series. How do we understand the range of types of time series?
- Calculate time series features. Using tsfeatures this will give 13 values for each time series. 
- Use scagnostics to find pairs of tsfeatures that are interesting, eg high on splines but low on monotonic
- Plot the pair of tsfeatures, what's unusual about the two? 
- Select unusual series from the tsfeatures, and plot

```{r getdata, eval=FALSE}
# This is the code that pulls the data together
# remotes::install_github("robjhyndman/compenginets")
library(compenginets)
library(tsfeatures)

# get 3 different types of time series
set.seed(300)
cets_birdsongs <- get_cets("Birdsong")
cets_finance <- get_cets("Finance")
cets_music <- get_cets("Music")
save(cets_birdsongs, file="data/cets_birdsongs.rda")
save(cets_finance, file="data/cets_finance.rda")
save(cets_music, file="data/cets_music.rda")

# compute time series features 
feat_birdsongs <- tsfeatures(cets_birdsongs) %>%
  mutate(name = names(cets_birdsongs), 
         type="birdsongs")
feat_finance <- tsfeatures(cets_finance) %>%
  mutate(name = names(cets_finance), 
         type="finance")
feat_music <- tsfeatures(cets_music) %>%
  mutate(name = names(cets_music), 
         type="music")
save(feat_birdsongs, file="data/feat_birdsongs.rda")
save(feat_finance, file="data/feat_finance.rda")
save(feat_music, file="data/feat_music.rda")

# make big dataset
features_bfm <- bind_rows(feat_birdsongs,
                          feat_finance,
                          feat_music)
```

```{r tscollections, eval=FALSE}
load("data/feat_birdsongs.rda")

# calculate scagnostics
scag_birdsongs <- calc_scags_wide(feat_birdsongs[,4:16])
```

```{r scatmat, fig.height=12, fig.width=12, out.width="100%", include=knitr::is_html_output(), eval=knitr::is_html_output(), eval=FALSE}
# interactive scatterplot 
scag_birdsongs <- scag_birdsongs %>%
  mutate(id = paste(Var1, Var2))

ggplotly(ggpairs(scag_birdsongs, columns = 3:10, aes(label=id)), width = 600, height = 600)
```

```{r scatmatstatic, fig.height=10, fig.width=10, out.width="100%", include=knitr::is_latex_output(), eval=knitr::is_latex_output(), eval=FALSE}
scag_birdsongs %>% 
  ggpairs(columns = 3:10)
```

<!--
- Maybe curvature and linearity are interesting? Have relatively higher splines than monotonic. 
- Plot the linearity against curvature. It's mostly a sprinkling of outliers. (Note, now, outlying scagnostic also has high value.) Select outlier which is highest on curvature but relatively low on linearity-->

```{r interesting-pair, eval=FALSE}
p <- ggplot(feat_birdsongs, aes(x=linearity, y=curvature, label = name)) +
  geom_point()
```

```{r interesting-pair-interactive, out.width="50%", include=knitr::is_html_output(), eval=knitr::is_html_output(), eval=FALSE}
ggplotly(p, width=300, height=300)
```

```{r interesting-pair-static, out.width="50%", include=knitr::is_latex_output(), eval=knitr::is_latex_output(), eval=FALSE}
p
```

```{r interesting-ts, eval=FALSE}
# plot the time series 
load("data/cets_birdsongs.rda")
load("data/feat_birdsongs.rda")

# Find interesting series: high curvature, low linearity
s <- feat_birdsongs %>% filter(curvature > 0.9) %>% select(name)
s_ts <- cets_birdsongs[[s$name]] %>%
  as_tibble() %>% mutate(t = 1:n())
ggplot(s_ts, aes(x=t, y=x)) + geom_line()
# Find interesting series: low curvature, high linearity
s <- feat_birdsongs %>% filter(linearity > 1.5) %>% select(name)
s_ts <- cets_birdsongs[[s$name]] %>%
  as_tibble() %>% mutate(t = 1:n())
ggplot(s_ts, aes(x=t, y=x)) + geom_line()
```


```{r interesting-pair2, eval=FALSE}
p <- ggplot(feat_birdsongs, aes(x=diff1_acf10, y=trend, label = name)) +
  geom_point()
```

```{r interesting-pair-interactive2, out.width="50%", include=knitr::is_html_output(), eval=knitr::is_html_output(), eval=FALSE}
ggplotly(p, width=300, height=300)
```

```{r interesting-pair-static2, out.width="50%", include=knitr::is_latex_output(), eval=knitr::is_latex_output(), eval=FALSE}
p
```

```{r interesting-ts2, eval=FALSE}
# plot the time series 
# Find interesting series: high trend, low diff1
s <- feat_birdsongs %>% filter(trend > 0.15) %>% select(name)
s_ts <- cets_birdsongs[[s$name]] %>%
  as_tibble() %>% mutate(t = 1:n())
ggplot(s_ts, aes(x=t, y=x)) + geom_line()
```

### Compare two sets of time series

<!-- I think that this is a promising use of scagnostics. This reveals a big difference between the three types is in sparse and splines, which points to a few of the tsfeatures including curvature and entropy. 

Need to refine this a lot more, and maybe just focus on differences between the three groups on each scagnostic. 

(Maybe also think about including a climate time series?)-->

This analysis compares the features of macroeconomic and microeconomic series, using scagnostics. The goal of the comparison is to compare shapes, not necessarily centres of groups as might be done in LDA or other machine learning methods. 

Here, just a small set of features is examined (because code fragile) but what emerges as interesting is the difference between curvature and trend strength. Microeconomic series tend to have high values on trend strength, and a range of values on curvature. In comparison macroeconomic series tend to have near constant average values on curvature, and highly varied on trend strength. 

Plotting a few series actually suggests that the microeconomic series contain lots of micro structure, which might be what we should expect. Interestingly the trend strength seems to pick up the jaggies!

```{r tscompare, eval=FALSE}
load("data/feat_finance.rda")
load("data/feat_music.rda")
feat_all <- bind_rows(feat_birdsongs, 
                      feat_finance,
                      feat_music)

# calculate scagnostics
# Would like to do this calculation using a group_by type statement
scag_birdsongs <- calc_scags_wide(feat_birdsongs[,4:16]) %>% 
  mutate(type = "birdsongs")
scag_finance <- calc_scags_wide(feat_finance[,4:16]) %>% 
  mutate(type = "finance")
scag_music <- calc_scags_wide(feat_music[,4:16]) %>% 
  mutate(type = "music")
scag_all <- bind_rows(scag_birdsongs, 
                      scag_finance) #,
                      scag_music)

scag_all <- scag_all %>%
  mutate(id = paste(Var1, Var2, type))

ggplotly(ggpairs(scag_all, columns = 3:10, aes(colour = type, label = id), alpha = 0.5), width = 800, height = 800)

p <- ggplot(feat_all, 
            aes(x=entropy, 
                y=curvature, 
                colour=type, 
                label=name)) + 
  geom_point()
ggplotly(p, width=500, height=300)

load("data/feat_finance.rda")
load("data/cets_finance.rda")

s <- feat_finance %>% filter(curvature > 45) %>% select(name)
s_ts <- cets_finance[[s$name]] %>%
  as_tibble() %>% mutate(t = 1:n())
ggplot(s_ts, aes(x=t, y=x)) + geom_line()

```

```{r eval=FALSE}
# Testing for Ursula's code
library(feasts)
get_features <- function(ts_in) {
  features(as_tsibble(ts_in), 
           value, feature_set("feasts"))
}
feats_birdsong <- purrr::map_dfr(cets_birdsongs, get_features)



cassowaryr::calc_scags_wide(feats_birdsong)

library(cassowaryr)
load("data/feasts_birdsong.rda")
sc_skinny(feats_birdsong$zero_run_mean, feats_birdsong$spikiness)
debug(cassowaryr:::sc_convex.list)


```


```{r eval=FALSE}
library(compenginets)

# Playing with other ideas
cets_macro <- get_cets("macroeconomics")
cets_micro <- get_cets("micoeconomics")
save(cets_macro, file="data/cets_macro.rda")
save(cets_micro, file="data/cets_micro.rda")

# Comparing two types of time series
library(feasts)
get_features <- function(ts_in) {
  features(as_tsibble(ts_in), 
           value, feature_set("feasts"))
}

load("data/cets_macro.rda")
feats_macro <- purrr::map_dfr(cets_macro, get_features)
save(feats_macro, file="data/feats_macro.rda")

load("data/cets_micro.rda")
feats_micro <- purrr::map_dfr(cets_micro, get_features)
save(feats_micro, file = "data/feats_micro.rda")
```

```{r}
load("data/feats_macro.rda")
load("data/feats_micro.rda")

# Check scale of each variable
#feats_macro %>% 
#  summarise_all(sd, na.rm=TRUE) %>% 
#  glimpse()

scags_macro <- calc_scags_wide(feats_macro[,1:4], 
  scags = c("convex", "splines", "skinny",
            "outlying", "stringy", "striated",
            "clumpy","sparse", "skewed"))

#feats_micro %>% 
#  summarise_all(sd, na.rm=TRUE) %>% 
#  glimpse()

scags_micro <- calc_scags_wide(feats_micro[,1:4], 
  scags = c("convex", "splines", "skinny",
            "outlying", "stringy", "striated",
            "clumpy","sparse", "skewed"))

scags_macro <- scags_macro %>%
  pivot_longer(outlying:splines, 
               names_to = "scags",
               values_to = "macro_value") 

scags_micro <- scags_micro %>%
  pivot_longer(outlying:splines, 
               names_to = "scags",
               values_to = "micro_value") 

scags_mac_mic <- full_join(scags_macro, scags_micro, by = c("Var1", "Var2", "scags"))
scags_mac_mic <- scags_mac_mic %>%
  mutate(scag_dif = abs(macro_value-micro_value)) %>%
           # *(macro_value+micro_value)) %>% # weight lower if both values are small
  arrange(desc(scag_dif)) %>%
  head(5)

feats_mac_mic <- bind_rows(
  mutate(feats_macro, type = "macro"),
  mutate(feats_micro, type = "micro")
)

p1 <- ggplot(feats_mac_mic, 
       aes(x=curvature, y=spikiness, colour = type)) + 
  geom_point(alpha = 0.5) +
  scale_colour_brewer("", palette="Dark2") +
  theme(aspect.ratio=1) + 
  ggtitle("Boring") # So boring

p2 <- ggplot(feats_mac_mic, 
       aes(x=curvature, y=trend_strength, colour = type)) + 
  geom_point(alpha = 0.5) +
  scale_colour_brewer("", palette="Dark2") +
  theme(aspect.ratio=1) + 
  ggtitle("Interesting") # Interesting

p3 <- ggplot(feats_mac_mic, 
       aes(x=curvature, y=linearity, colour = type)) + 
  geom_point(alpha = 0.5) +
  scale_colour_brewer("", palette="Dark2") +
  theme(aspect.ratio=1) + 
  ggtitle("Interesting?") # Sort of interesting

p1 + p2 + p3 + plot_layout(guides = "collect") & theme(legend.position = "bottom")
```

```{r}
load("data/cets_macro.rda")
load("data/cets_micro.rda")

macro_s_ts1 <- cets_macro[[1]] %>%
  as_tibble() %>% 
  mutate(x = as.numeric(x), 
         t = 1:n(), 
         name = names(cets_macro)[[1]])
macro_s_ts2 <- cets_macro[[2]] %>%
  as_tibble() %>% 
  mutate(x = as.numeric(x), 
         t = 1:n(), 
         name = names(cets_macro)[[2]])
macro_s_ts3 <- cets_macro[[3]] %>%
  as_tibble() %>% 
  mutate(x = as.numeric(x), 
         t = 1:n(), 
         name = names(cets_macro)[[3]])
macro_s_ts <- bind_rows(macro_s_ts1,
                        macro_s_ts2,
                        macro_s_ts3)
mac <- ggplot(macro_s_ts, aes(x=t, y=x)) +
  geom_line() + 
  facet_wrap(~name, ncol=1, scales = "free") +
  ggtitle("Macroeconomics") +
  theme(axis.text = element_blank())

micro_s_ts1 <- cets_micro[[1]] %>%
  as_tibble() %>% 
  mutate(x = as.numeric(x), 
         t = 1:n(), 
         name = names(cets_micro)[[1]])
micro_s_ts2 <- cets_micro[[2]] %>%
  as_tibble() %>% 
  mutate(x = as.numeric(x), 
         t = 1:n(), 
         name = names(cets_micro)[[2]])
micro_s_ts3 <- cets_micro[[3]] %>%
  as_tibble() %>% 
  mutate(x = as.numeric(x), 
         t = 1:n(), 
         name = names(cets_micro)[[3]])
micro_s_ts <- bind_rows(micro_s_ts1,
                        micro_s_ts2,
                        micro_s_ts3)
mic <- ggplot(micro_s_ts, aes(x=t, y=x)) + 
  geom_line() + 
  facet_wrap(~name, ncol=1, scales = "free") +
  ggtitle("Microeconomics") +
  theme(axis.text = element_blank())

mac + mic

```

## Parkinsons 

## Black hole mergers

This is a simulated dataset that contains posterior samples for describing an observed gravitational waves signal from a black hole merger in terms of position in the sky (ra, dec, distance), time of the event (time) and the black hole properties (masses m1 and m2; spin related properties alpha, theta_jn, chi_tot, chi_eff, chi_p) and additional nuisance parameters psi (polarisation angle) and phi_jl (orbital phase). There are thus 13 variables and it is still feasible to look at a complete SPLOM, providing a good cross check of the scagnostics.

The data contains 9998 posterior samples, without binning it is too long to compute the scagnostics on such a large number of observations. For our purpose a much smaller sample is sufficient, and we randomly sample 200 observations before computing the scagnostics.

```{r eval=FALSE}
set.seed(26)
bbh <- read_csv("data/bbh_posterior_samples.csv") %>%
  sample_n(200)
scag_bbh <- calc_scags_wide(bbh)

bbh1 <- ggplot(scag_bbh, aes(x=convex, y=skinny, 
                      label = paste(Var1, Var2))) +
  geom_point()
bbh2 <- ggplot(scag_bbh, aes(x=dcor, y=splines, 
                      label = paste(Var1, Var2))) +
  geom_point()
bbh3 <- ggplot(scag_bbh, aes(x=clumpy, y=skewed, 
                      label = paste(Var1, Var2))) +
  geom_point()
gs1 <- ggplotly(bbh1)
gs2 <- ggplotly(bbh2)
gs3 <- ggplotly(bbh3)
subplot(gs1, gs2, gs3, nrows=1, widths = c(0.33, 0.33, 0.33), heights = 0.6)
```

Combinations that stand out: time-ra, dec-ra, dec-time (low convex, high skinny)
dec-ra and time-ra also have higher splines than dcor (both high, non-linear functional relation), while m1-m2, dec-time and chi_p-chi_tot have higher dcor than splines (still both high, m1-m2 and chi_p-chi_tot are linear relations with noise, dec-time is strong association but not function)
The final plot is showing clumpy vs skewed, shows that clumpy isn't really doing what we expect since we would expect much more structure in clumpy (in particular plots with time break up into two well separated groups, plots with ra in three separated groups, some other variables introduce less pronounced separation between groups). Included time-alpha as one example, this has clumpy of 0.9 and skewed of 0.7.

```{r eval=FALSE}

bbh1 <- ggplot(bbh, aes(x=time, y=ra)) +
  geom_point()
bbh2 <- ggplot(bbh, aes(x=dec, y=ra)) +
  geom_point()
bbh3 <- ggplot(bbh, aes(x=dec, y=time)) +
  geom_point()
bbh4 <- ggplot(bbh, aes(x=m1, y=m2)) +
  geom_point()
bbh5 <- ggplot(bbh, aes(x=chi_p, y=chi_tot)) +
  geom_point()
bbh6 <- ggplot(bbh, aes(x=time, y=alpha)) +
  geom_point()

subplot(bbh1, bbh2, bbh3, bbh4, bbh5, bbh6,
        nrows=2, widths = c(0.33, 0.33, 0.33), heights = c(0.5, 0.5))
```

## AFL player statistics
Some explanation of AFL, and about the AFLW competition. Particularly explain any stats used in the plots: goals, kicks, posessions, ...

The Australian Football League Women's (AFLW) is the national semi-profesisonal Australia Rules football league for female players. Here we will analyse data sourced from the official AFL website with information on the 2020 season, in which the league had 14 teams and 1932 players. There are 68 variables, 38 of which are numeric. The others are categorical, like the players names or match ids, which would not be used in scagnostic calculations. These numeric variables are recorded per player per game:
- *timeOnGroundPercentage*: percentage of the game the player was on the field.  
- *goals*: the 6 points a team gets when the kick the ball between the two big posts.  
- *behinds*: the 1 point a team gets when they kick the ball between the big post and small post.  
- *kicks*: number of kicks done by the player in this game.  
- *handballs*: number of handballs does by the player in the game.  
- *disposals*: the number kicks and handballs a player has.  
- *marks*: total number of marks in the game (the ball travels more than 15m and the player catches it without another player touching it or it hitting the ground).  
- *bounces*: the number of times a player bounced the ball in a game. A player must bounce the ball if they travel more than 15m and they can only bounce the ball once.  
- *tackles*: Number of tackles performed by the player.  
- *contestedPossessions*: the number of disposals a player has under pressure, i.e if a player is getting tackled and the get a handball or kick out of the scuffle.  
- *uncontestedPossessions*: the number of disposals a player has under no pressure where they have space and time to get rid of the ball.  
- *totalPossessions*: The total number of time the player has the ball. 
- *inside50s*: the number of times the player has the ball within the 50m arc around the oponents goals.  
- *marksInside50*: the number of marks a player gets within the 50m arc around the oponents goals.  
- *contestedMarks*: the number of marks a player has under pressure.
- *hitouts*: this is how many times a player or team taps or punching the ball from a stoppage.
- *onePercenters*: all the things a player can do without registering a disposal. Eg. Spoils (punching the ball to stop someone from marking it), Shepparding (blocking for a teammate), smothering.  
disposalEfficiency: a measure of how well a player disposes of the ball. E.g. if a player kicks or handballs to the opposition a lot, they will have a low disposal efficiency percentage.   
- *clangers*: this is how many times a player or team dispose of the ball and it results in a turnover to the other team.  
- *freesFor*: this player was awarded a free kick.  
- *freesAgainst*:  this player caused a free kick to be awarded to the other team.  
- *dreamTeamPoints*: this is fantasy football scoring points.  
- *rebound50s*: how many times the player exits the ball out of their defence 50m arc.  
- *goalAssists*: number of times the player gave the pass immediately before the player that scored a goal.
- *goalAccuracy*: percentage ratio of the number of goals kicked to the number of goal attempts.  
- *turnovers*: this players disposal caused a turnover (the ball touches the ground and the other team get it).  
- *intercepts*: number of times this player intercepts the disposal of the other team.
- *tacklesInside50*: number of tackles performed by this player within their defence 50m arc.  
- *shotsAtGoal*: number of total shots at goal for this player (sum of goals, behinds and misses)
- *scoreInvolvements*: number of times the player was involved in a passage of play leading up to a goal.
- *metresGained*:  how far a player has been able to advance the ball without turning it over.  
- *clearances.centreClearances*: this is the clearance from the centre bounce after a goal or at the start of a quarter
- *clearances.stoppageClearances*: all the clearance from stoppages around the ground
- *clearances.totalClearances*: how many time a player or team clears the ball from a stoppage or from the centre


With 38 variables, there are 703 possible scatterplots to make. The scagnostics can suggest which are the interesting ones to examine. 

```{r eval=FALSE}
library(fitzRoy)
aflw <- fetch_player_stats(2020, comp = "AFLW")
save(aflw, file = "data/aflw.rda")

aflw_num <- aflw %>%
  select_if(is.numeric)
save(aflw_num, file = "data/aflw_num.rda")

scag_aflw <- calc_scags_wide(aflw_num[,5:37])
save(scag_aflw, file = "data/scagnostics_aflw.rda")

```

```{r splines}
# Saved scagnostics because calc takes about 30 mins
load("data/scagnostics_aflw.rda")
load("data/aflw_num.rda")
load("data/aflw.rda")

# Highest splines table
scag_aflw %>% 
  select(Var1, Var2, splines) %>%
  arrange(desc(splines)) %>%
  head(10) %>% 
  kable(digits=2)

# High on splines
aflw <- aflw %>%
  mutate(name = paste(player.givenName, player.surname))
s1 <- ggplot(aflw, 
             aes(x=totalPossessions, 
                 y=disposals, 
                 label = name)) +
  geom_abline(intercept = 0, slope = 1) +
  geom_point() 
s2 <- ggplot(aflw, 
             aes(x=goals, 
                 y=goalAccuracy, 
                 label = name)) +
  geom_jitter(width = 0.1, height = 2) 
s3 <- ggplot(aflw, 
             aes(x=kicks, 
                 y=disposals, 
                 label = name)) +
  geom_point() 
```

```{r aflwinteractive, fig.cap="Scatterplots with high values on the splines scagnostic. Mouseover to examine the players relative the the statistics.", include=knitr::is_html_output(), eval=knitr::is_html_output()}
gs1 <- ggplotly(s1)
gs2 <- ggplotly(s2)
gs3 <- ggplotly(s3)
subplot(gs1, gs2, gs3, nrows=1, widths = c(0.33, 0.33, 0.33), heights = 0.6)
```

```{r aflwstatic, fig.cap="Scatterplots with high values on the splines scagnostic.", include=knitr::is_latex_output(), eval=knitr::is_latex_output()}
s1 + s2 + s3
```

Figure \@(fig:aflwinteractive) shows three scatterplots that score highly on the splines scagnostic. Each of these shows a relatively strong monotonic relationship between the two variables. In the interactive version of the plot, mouse over reveals some high-performing players, e.g. Anne Hatchard has a lot of possessions, disposals and kicks, and Kaitlyn Ashmore kicked 4 goals in a match with 100% accuracy.

NOTE: Each player is represented multiple times here, I think. The stats are per game. Maybe it is better to aggregate for each player and re-do the statistics?

```{r some_are_kickers, fig.cap = "Some players tend to kick the ball, even when challenged, whereas others more often use handball for disposals. ", include=knitr::is_html_output(), eval=knitr::is_html_output()}
# High on clumpy, low on striated
kickers <- ggplot(aflw, aes(x=contestedPossessions, y=handballs, 
                      label = name)) +
  geom_abline(intercept = 0, slope = 1) +
  geom_jitter()
ggplotly(kickers, width=400, height=400)
```

```{r eval=FALSE}
# Parallel coordinate plot
library(GGally)
ggparcoord(scag_aflw, columns = 3:11, scale = "globalminmax")

# Or look at pairs
ggplot(scag_aflw, aes(x=splines, y=striated, 
                      label = paste(Var1, Var2))) +
  geom_point()
ggplot(scag_aflw, aes(x=clumpy, y=striated, 
                      label = paste(Var1, Var2))) +
  geom_point()
ggplotly()

```

```{r eval=FALSE}
# Extra AFLW analysis, not to include
# High on outlying: actually not so interesting
out <- ggplot(aflw, aes(x=metresGained, y=hitouts)) +
  geom_point()

# High on clumpy
ggplot(aflw, aes(x=disposalEfficiency, y=bounces, label = name)) +
  #geom_abline(intercept = 0, slope = 1) +
  geom_point() 
ggplotly()
```

```{r eval=FALSE}
# Was thinking this might be a good example, but too many missings
library(NHANES)
NHANES_numeric <- NHANES %>%
  select_if(is.numeric)

# Lots of missing values, need to check
library(naniar)
vis_miss(NHANES_numeric)
vm <- miss_var_summary(NHANES_numeric[,-1])

# Keep only variables with less than 50% missing
keep <- vm %>% filter(pct_miss < 50)

# Now compute scagnostics
scag_nhanes <- calc_scags_wide(NHANES_numeric[,keep$variable])
```

```{r, eval=FALSE}
# highest on each

# Outlying (high)
ggplot(aflw, aes(x=	metresGained, y= bounces)) + geom_point() 
#metresGained has a large number of very close variables that take up most of the MST, and then a handful of higher up ones (it has outliers on this one variable). Anything with meters gained will be high on outlying
ggplot(aflw, aes(x=	dreamTeamPoints, y= contestedMarks)) + geom_point()

# Stringy
ggplot(aflw, aes(x=clearances.centreClearances, y=contestedMarks)) + geom_point() #lowest
ggplot(aflw, aes(x=metresGained, y=goalAssists)) + geom_point() #highest

# Striated
ggplot(aflw, aes(x=clearances.stoppageClearances, y=shotsAtGoal)) + geom_point() #lowest
#highest is same as stringy

#Striated adjusted
#discussed below

#Clumpy
ggplot(aflw, aes(x=behinds, y=goals)) + geom_point() #lowest
ggplot(aflw, aes(x=	metresGained, y= shotsAtGoal)) + geom_point() #highest

#clumpy_adjusted
cl_a <- calc_scags_wide(aflw_num[,5:37], scags="clumpy_adjusted")
ggplot(aflw, aes(x=metresGained, y=goalAssists)) + geom_point()  #highest
ggplot(aflw, aes(x=	clearances.centreClearances, y= behinds)) + geom_point() #lowest

#Sparse
ggplot(aflw, aes(x=	goalAssists, y= clangers)) + geom_point() #highest

#Skewed
ggplot(aflw, aes(x=	tacklesInside50, y= marksInside50)) + geom_point() #highest



```

The scagnsotics need to be used and interpreted with the type of dataset you are working with in mind. For example, since these are sports stats, almost all of the variables are discrete. This means in the case of the striated varibale, we would be interested in the scatter plots that are very low on striated rather than high. Lets see which striated measure can find some interesting scatter plots.

```{r, eval=FALSE}
# im just adding in striated_adjusted here to compare without recalculating the whole thing
new_scag_aflw <- scag_aflw
new_scag_aflw$striated_adjusted <- AFL_stri_adj$striated_adjusted #calc_scags_wide(aflw_num[,5:37], scags = "striated_adjusted")

scag_aflw %>% 
  select(Var1, Var2, striated) %>%
  arrange(desc(striated)) %>%
  head(10) 

# look at lowest values for adjusted striated value
lowest_stri <- new_scag_aflw[order(new_scag_aflw$striated),] %>%
  select(Var1, Var2, striated, striated_adjusted) %>%
  head(10)

lowest_stri_adjusted <- new_scag_aflw[order(new_scag_aflw$striated_adjusted),] %>%
  select(Var1, Var2, striated, striated_adjusted) %>%
  head(10)

ggplot(aflw, aes(x=metresGained, y=goals)) + geom_point()

#look at top 3 for striated
ggplot(aflw, aes(x=clearances.stoppageClearances, y=shotsAtGoal)) + geom_point()
ggplot(aflw, aes(x=contestedPossessions, y=	tackles)) + geom_point()
ggplot(aflw, aes(x=tacklesInside50, y=marksInside50)) + geom_point()
#not interesting

# look at top 3 for striated_adjusted
# two continuous variables
ggplot(aflw, aes(x=metresGained, y=dreamTeamPoints)) + geom_point()
# fractional plots, not interesting technically but they are interesting visually
ggplot(aflw, aes(x=goalAccuracy, y=shotsAtGoal)) + geom_point()
ggplot(aflw, aes(x=disposalEfficiency, y=disposals)) + geom_point()

#THE OTHER 6 PLOTS
ggplot(aflw, aes(x=metresGained, y=disposalEfficiency)) + geom_point() #fraction thing issue again
ggplot(aflw, aes(x=dreamTeamPoints, y=disposalEfficiency)) + geom_point()
# this is a plot that will have a higher striated value with more data
ggplot(aflw, aes(x=	goals, y= goalAccuracy)) + geom_point()
ggplot(aflw, aes(x=dreamTeamPoints, y=hitouts)) + geom_point() # 1 with more data
ggplot(aflw, aes(x=disposalEfficiency, y=hitouts)) + geom_point() #fraction and large discrete
ggplot(aflw, aes(x=totalPossessions, y=hitouts)) + geom_point() #two large discretes
ggplot(aflw, aes(x=hitouts, y=disposals)) + geom_point() #two large discretes
```
Why the lowest values on striated? This is a discrete data set, if all the points are not at right angles or in a stright line, to each other, they are not just randonly spread on the. Since the old striated measure is specifically trying to find a continuous variable against a discrete variable, its highest values are also identified by the striated_adjusted. The lowest values on striated simply identify a plot where all the variables are at right angles, once again a measure of disceteness but one that is not identified by striated. Striated_adjusted encapsulates both versions of discreteness in the values that get exactly a 1, this means the scatter plot that gets the lowest value should be the two variables that are continuous. Following that striated_adjusted gives some interesting scatter plots in both goal and dispsoal accuracy vs number. **comment on the scatter plots**.

## World Development Indicators

The World Bank delivers a lot of development indicators, for many countries and multiple years. This might be a good example to identify pairs of indicators with interesting relationships.

Download data from:

https://databank.worldbank.org/source/world-development-indicators/preview/on#

## Summary

